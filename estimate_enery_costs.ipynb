{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-11T13:40:29.393129Z",
     "start_time": "2025-09-11T13:40:28.829526Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import json, re\n",
    "import math\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:45:05.323388Z",
     "start_time": "2025-09-11T13:40:30.190432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check CSAS documents to get an idea of average doc length\n",
    "\n",
    "parsed_documents_folder = \"../Data/ParsedPublications/\"\n",
    "\n",
    "def create_word_counts(folder):\n",
    "    files = list(Path(folder).rglob('*.json'))\n",
    "    rows = []\n",
    "    for file_path in tqdm(files, desc=\"Processing documents\"):\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        name = data['name']\n",
    "        number_words = len(re.findall(r'\\b\\w+\\b', data.get('text', '')))\n",
    "        rows.append((name, number_words))\n",
    "    return pd.DataFrame(rows, columns=['name', 'number_words'])\n",
    "\n",
    "df = create_word_counts(parsed_documents_folder)"
   ],
   "id": "bc065f6bbe4acce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing documents:   0%|          | 0/12752 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6580a265ebbe4327bc85cf2af294c465"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:53:31.347081Z",
     "start_time": "2025-09-11T13:53:31.326591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "percentiles = [0.01, 0.1, 0.5, 0.9, 0.99]\n",
    "percentile_names = [\"mean\", \"1%\", \"10%\", \"50%\", \"90%\", \"99%\"]\n",
    "\n",
    "print(percentile_names)\n",
    "print(\n",
    "    [int(round(x, -3)) for x in df.describe(percentiles=percentiles).loc[percentile_names, 'number_words'].to_list()]\n",
    ")"
   ],
   "id": "151ab45298203004",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean', '1%', '10%', '50%', '90%', '99%']\n",
      "[12000, 1000, 3000, 8000, 25000, 67000]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:55:19.671539Z",
     "start_time": "2025-09-11T13:55:19.665761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "percentiles = [0.1, 0.9]\n",
    "percentile_names = [\"10%\", \"mean\", \"90%\"]\n",
    "word_count_list = [int(round(x, -3)) for x in df.describe(percentiles=percentiles).loc[percentile_names, 'number_words'].to_list()] \n",
    "\n",
    "print(percentile_names)\n",
    "print(word_count_list)"
   ],
   "id": "11bf5f13a6e171f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10%', 'mean', '90%']\n",
      "[3000, 12000, 25000]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:55:27.144980Z",
     "start_time": "2025-09-11T13:55:27.133114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Token <-> word: 1 token ≈ 3/4 of a word → tokens ≈ words / 0.75  [OpenAI help]\n",
    "# https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "TOKENS_PER_WORD = 1/0.75\n",
    "\n",
    "# Device power under load (RTX 5000 Ada max power 250 W) [NVIDIA product page]\n",
    "# https://www.nvidia.com/en-us/products/workstations/rtx-5000/\n",
    "DEVICE_POWER_WATTS = 250.0\n",
    "\n",
    "# Model families (for context/size; smallest is Opus-MT among listed)\n",
    "# mBART-50 ~610M params [HF docs]: https://huggingface.co/transformers/v4.11.3/pretrained_models.html\n",
    "# M2M100_418M = 418M params [HF card]: https://huggingface.co/facebook/m2m100_418M\n",
    "# Opus-MT is MarianMT (6e/6d “base” style; size << mBART-50/M2M100) [HF Marian docs]\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/marian\n",
    "\n",
    "# Throughput (tokens/sec) — engineering assumptions; replace with your measurements.\n",
    "# No canonical source; depends on batch size, decoding params, kernels, driver, etc.\n",
    "TPS = {\n",
    "    # Opus-MT is smallest here; start with a conservative high throughput.\n",
    "    \"opus\": 1500.0,      # assumption: adjust if you measure differently\n",
    "    \"m2m100_418m\": 600.0,  # assumption\n",
    "    \"mbart50_mmt\": 400.0,  # assumption\n",
    "}\n",
    "\n",
    "# Overhead for dict lookups + embedding similarity + comparison in “all models” scenario\n",
    "# (tiny vs GPU decode; add 5–10% buffer). Choose midpoint = 7.5%. [engineering assumption]\n",
    "OVERHEAD_FRACTION = 0.075\n"
   ],
   "id": "894c76f11df66e28",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:18:39.082053Z",
     "start_time": "2025-09-11T14:18:39.076543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def words_to_tokens(words):\n",
    "    return math.ceil(words * TOKENS_PER_WORD)\n",
    "\n",
    "def energy_time_for_model(tokens, tps, power_watts=DEVICE_POWER_WATTS):\n",
    "    t_sec = tokens / tps\n",
    "    e_Wh = power_watts * (t_sec / 3600.0)\n",
    "    return e_Wh, t_sec\n",
    "\n",
    "def scenario_smallest_only(words, tps_opus=TPS[\"opus\"]):\n",
    "    tokens = words_to_tokens(words)\n",
    "    return energy_time_for_model(tokens, tps_opus)\n",
    "\n",
    "def scenario_all_models(words, tps=TPS, overhead=OVERHEAD_FRACTION):\n",
    "    tokens = words_to_tokens(words)\n",
    "    # 12 models per direction: 4 Opus variants, 4 M2M100 variants, 4 mBART-50 variants\n",
    "    groups = {\n",
    "        \"opus\": 4,\n",
    "        \"m2m100_418m\": 4,\n",
    "        \"mbart50_mmt\": 4,\n",
    "    }\n",
    "    total_e_Wh = 0.0\n",
    "    total_t_sec = 0.0\n",
    "    for k, n in groups.items():\n",
    "        e, t = energy_time_for_model(tokens, tps[k])\n",
    "        total_e_Wh += n * e\n",
    "        total_t_sec += n * t\n",
    "    total_e_Wh *= (1.0 + overhead)\n",
    "    total_t_sec *= (1.0 + overhead)\n",
    "    return total_e_Wh, total_t_sec\n",
    "\n",
    "def summarize(words_list=(1000, 10000)):\n",
    "    cost_per_kWh = 0.10\n",
    "    cost_per_Wh = cost_per_kWh / 1000.0\n",
    "\n",
    "    rows = []\n",
    "    for words in words_list:\n",
    "        # scenario 1: smallest only (Opus)\n",
    "        e1, t1 = scenario_smallest_only(words)\n",
    "        rows.append({\n",
    "            \"words\": words,\n",
    "            \"scenario\": \"smallest_only (Opus-MT)\",\n",
    "            \"time_sec\": round(t1, 1),\n",
    "            \"energy_Wh\": round(e1, 2),\n",
    "            \"cost_$\": round(e1 * cost_per_Wh, 5),\n",
    "        })\n",
    "\n",
    "        # scenario 2: all models\n",
    "        e2, t2 = scenario_all_models(words)\n",
    "        rows.append({\n",
    "            \"words\": words,\n",
    "            \"scenario\": \"all_models + best_similarity (12 models)\",\n",
    "            \"time_sec\": round(t2, 1),\n",
    "            \"energy_Wh\": round(e2, 2),\n",
    "            \"cost_$\": round(e2 * cost_per_Wh, 5),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ],
   "id": "9fad6857c22cfa6e",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:18:39.602092Z",
     "start_time": "2025-09-11T14:18:39.589746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = summarize(word_count_list)\n",
    "display(df)"
   ],
   "id": "69dc091f90966af1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   words                                  scenario  time_sec  energy_Wh  \\\n",
       "0   3000                   smallest_only (Opus-MT)       2.7       0.19   \n",
       "1   3000  all_models + best_similarity (12 models)      83.1       5.77   \n",
       "2  12000                   smallest_only (Opus-MT)      10.7       0.74   \n",
       "3  12000  all_models + best_similarity (12 models)     332.5      23.09   \n",
       "4  25000                   smallest_only (Opus-MT)      22.2       1.54   \n",
       "5  25000  all_models + best_similarity (12 models)     692.8      48.11   \n",
       "\n",
       "    cost_$  \n",
       "0  0.00002  \n",
       "1  0.00058  \n",
       "2  0.00007  \n",
       "3  0.00231  \n",
       "4  0.00015  \n",
       "5  0.00481  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>scenario</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>energy_Wh</th>\n",
       "      <th>cost_$</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>smallest_only (Opus-MT)</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000</td>\n",
       "      <td>all_models + best_similarity (12 models)</td>\n",
       "      <td>83.1</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.00058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>smallest_only (Opus-MT)</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12000</td>\n",
       "      <td>all_models + best_similarity (12 models)</td>\n",
       "      <td>332.5</td>\n",
       "      <td>23.09</td>\n",
       "      <td>0.00231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25000</td>\n",
       "      <td>smallest_only (Opus-MT)</td>\n",
       "      <td>22.2</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25000</td>\n",
       "      <td>all_models + best_similarity (12 models)</td>\n",
       "      <td>692.8</td>\n",
       "      <td>48.11</td>\n",
       "      <td>0.00481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:26:30.722567Z",
     "start_time": "2025-09-11T14:26:30.705437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # If we have terrible hardware or an incorrectly configured pipeline, what is the worst-case energy consumption?\n",
    "# \n",
    "# Worst-case multipliers\n",
    "# \n",
    "# Hardware\n",
    "#   Switch to CPU-only (very old Xeon, no AVX2): 100–200x slower vs GPU.\n",
    "#   Or use an ancient GPU (Kepler K20, ~3–4 TFLOPS, 225 W): ~50x slower vs Ada. \n",
    "#   (CPU-only is actually worse here.)\n",
    "# \n",
    "# Precision\n",
    "#  Force FP32 instead of fp16/int8: ~2–3x slower.\n",
    "# \n",
    "# Decoding\n",
    "#   Large beam search (width 20+): ~10x slowdown vs greedy.\n",
    "#   Enable nucleus sampling with small cutoff, long tail: ~2x.\n",
    "#   Disable batching (one sentence at a time): ~2x.\n",
    "# \n",
    "# Facility\n",
    "#   Poor datacenter PUE (Power Usage Effectiveness) = 2.0 (vs efficient 1.2): ~1.7× overhead.\n",
    "# \n",
    "# Total worst-case multiplier ≈ 150 * 2.5 * 20 * 2 * 2 * 2 ≈ 60,000x\n",
    "# \n",
    "# Most realistic worst-case is using a CPU only ≈ 150x\n",
    "\n",
    "n_csas_documents_low = 50\n",
    "n_csas_documents_normal = 130 \n",
    "n_csas_documents_high = 200\n",
    "\n",
    "print(f\"Yearly CSAS Translation Costs for Translating between {n_csas_documents_low} and {n_csas_documents_high} documents\")\n",
    "\n",
    "print(\"\\nAssuming Properly Configured GPU and Pipeline\")\n",
    "print(f\"Best case costs = ${n_csas_documents_low * 0.00007:.2f}\")\n",
    "print(f\"Worst case costs = ${n_csas_documents_high * 0.00231:.2f}\")\n",
    "print(f\"Best guess costs = ${n_csas_documents_low * (0.00007 + 0.00231) / 2:.2f}\")\n",
    "\n",
    "print(\"\\nAssuming Poorly Configured Computer (CPU only)\")\n",
    "print(f\"Best case costs = ${150 * n_csas_documents_low * 0.00007:.2f}\")\n",
    "print(f\"Worst case costs = ${150 * n_csas_documents_high * 0.00231:.2f}\")\n",
    "print(f\"Best guess costs = ${150 * n_csas_documents_low * (0.00007 + 0.00231) / 2:.2f}\")"
   ],
   "id": "d6edf1632439e8fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yearly CSAS Translation Costs for Translating between 50 and 200 documents\n",
      "\n",
      "Assuming Properly Configured GPU and Pipeline\n",
      "Best case costs = $0.00\n",
      "Worst case costs = $0.46\n",
      "Best guess costs = $0.06\n",
      "\n",
      "Assuming Poorly Configured Computer (CPU only)\n",
      "Best case costs = $0.52\n",
      "Worst case costs = $69.30\n",
      "Best guess costs = $8.93\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# how much does it cost just to leave a computer on? \n",
    "# 60–80 W * 24 h/day * 365 ≈ 500–700 kWh/year\n",
    "#  $50-70/yr\n",
    "# \n",
    "#  this is the dominant cost"
   ],
   "id": "61638de5160cd83d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
